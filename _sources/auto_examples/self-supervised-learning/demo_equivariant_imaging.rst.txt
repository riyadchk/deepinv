
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-22

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.training_utils import train, test
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 23-26

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 26-39

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 40-49

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 49-63

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      2%|▏         | 6.76M/399M [00:00<00:05, 67.6MiB/s]      4%|▍         | 15.1M/399M [00:00<00:04, 76.8MiB/s]      6%|▌         | 23.4M/399M [00:00<00:04, 79.7MiB/s]      8%|▊         | 31.7M/399M [00:00<00:04, 80.9MiB/s]     10%|█         | 40.0M/399M [00:00<00:04, 81.7MiB/s]     12%|█▏        | 48.3M/399M [00:00<00:04, 82.2MiB/s]     14%|█▍        | 56.7M/399M [00:00<00:04, 82.6MiB/s]     16%|█▋        | 65.0M/399M [00:00<00:04, 83.0MiB/s]     18%|█▊        | 73.3M/399M [00:00<00:03, 82.7MiB/s]     20%|██        | 81.6M/399M [00:01<00:03, 82.9MiB/s]     23%|██▎       | 90.0M/399M [00:01<00:03, 83.1MiB/s]     25%|██▍       | 98.3M/399M [00:01<00:03, 83.1MiB/s]     27%|██▋       | 107M/399M [00:01<00:03, 83.2MiB/s]      29%|██▉       | 115M/399M [00:01<00:03, 83.3MiB/s]     31%|███       | 123M/399M [00:01<00:03, 83.2MiB/s]     33%|███▎      | 132M/399M [00:01<00:03, 83.3MiB/s]     35%|███▌      | 140M/399M [00:01<00:03, 83.3MiB/s]     37%|███▋      | 148M/399M [00:01<00:03, 83.3MiB/s]     39%|███▉      | 157M/399M [00:01<00:02, 83.1MiB/s]     41%|████▏     | 165M/399M [00:02<00:02, 82.7MiB/s]     43%|████▎     | 173M/399M [00:02<00:02, 82.3MiB/s]     46%|████▌     | 182M/399M [00:02<00:02, 82.3MiB/s]     48%|████▊     | 190M/399M [00:02<00:02, 82.5MiB/s]     50%|████▉     | 198M/399M [00:02<00:02, 82.8MiB/s]     52%|█████▏    | 206M/399M [00:02<00:02, 82.8MiB/s]     54%|█████▍    | 215M/399M [00:02<00:02, 82.9MiB/s]     56%|█████▌    | 223M/399M [00:02<00:02, 83.0MiB/s]     58%|█████▊    | 231M/399M [00:02<00:02, 82.8MiB/s]     60%|██████    | 240M/399M [00:02<00:01, 82.6MiB/s]     62%|██████▏   | 248M/399M [00:03<00:01, 82.1MiB/s]     64%|██████▍   | 256M/399M [00:03<00:01, 82.4MiB/s]     66%|██████▋   | 265M/399M [00:03<00:01, 82.6MiB/s]     68%|██████▊   | 273M/399M [00:03<00:01, 82.6MiB/s]     71%|███████   | 281M/399M [00:03<00:01, 82.6MiB/s]     73%|███████▎  | 289M/399M [00:03<00:01, 82.7MiB/s]     75%|███████▍  | 298M/399M [00:03<00:01, 82.7MiB/s]     77%|███████▋  | 306M/399M [00:03<00:01, 82.7MiB/s]     79%|███████▉  | 314M/399M [00:03<00:01, 82.6MiB/s]     81%|████████  | 322M/399M [00:03<00:00, 82.7MiB/s]     83%|████████▎ | 331M/399M [00:04<00:00, 82.3MiB/s]     85%|████████▌ | 339M/399M [00:04<00:00, 82.4MiB/s]     87%|████████▋ | 347M/399M [00:04<00:00, 82.4MiB/s]     89%|████████▉ | 356M/399M [00:04<00:00, 82.6MiB/s]     91%|█████████▏| 364M/399M [00:04<00:00, 82.7MiB/s]     93%|█████████▎| 372M/399M [00:04<00:00, 82.9MiB/s]     95%|█████████▌| 381M/399M [00:04<00:00, 83.2MiB/s]     98%|█████████▊| 389M/399M [00:04<00:00, 83.2MiB/s]    100%|█████████▉| 397M/399M [00:04<00:00, 83.3MiB/s]    100%|██████████| 399M/399M [00:04<00:00, 82.6MiB/s]




.. GENERATED FROM PYTHON SOURCE LINES 64-68

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 68-98

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 119.60it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/19 [00:00<?, ?it/s]    100%|██████████| 19/19 [00:00<00:00, 193.64it/s]
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 99-104

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 104-149

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            train=True,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 150-163

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 163-188

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]      8%|▊         | 176k/2.17M [00:00<00:01, 1.57MB/s]     15%|█▌        | 336k/2.17M [00:00<00:01, 1.14MB/s]     31%|███       | 688k/2.17M [00:00<00:00, 2.03MB/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 4.72MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 189-193

Train the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 193-221

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    train(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        ckp_interval=10,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
      0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:02<?, ?it/s, eval_psnr=39.7, loss_mc=2.08e-5, loss_ei=0.000125, total_loss=0.000146, train_psnr=38.2]    Epoch 1:  20%|██        | 1/5 [00:02<00:08,  2.17s/it, eval_psnr=39.7, loss_mc=2.08e-5, loss_ei=0.000125, total_loss=0.000146, train_psnr=38.2]    Epoch 1:  20%|██        | 1/5 [00:02<00:08,  2.17s/it, eval_psnr=39.7, loss_mc=2.08e-5, loss_ei=0.000125, total_loss=0.000146, train_psnr=38.2]    Epoch 1:  20%|██        | 1/5 [00:04<00:08,  2.17s/it, eval_psnr=39.7, loss_mc=1.51e-5, loss_ei=9.22e-5, total_loss=0.000107, train_psnr=39.2]     Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.16s/it, eval_psnr=39.7, loss_mc=1.51e-5, loss_ei=9.22e-5, total_loss=0.000107, train_psnr=39.2]    Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.16s/it, eval_psnr=39.7, loss_mc=1.51e-5, loss_ei=9.22e-5, total_loss=0.000107, train_psnr=39.2]    Epoch 1:  40%|████      | 2/5 [00:06<00:06,  2.16s/it, eval_psnr=39.7, loss_mc=1.28e-5, loss_ei=9.62e-5, total_loss=0.000109, train_psnr=39.2]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.17s/it, eval_psnr=39.7, loss_mc=1.28e-5, loss_ei=9.62e-5, total_loss=0.000109, train_psnr=39.2]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.17s/it, eval_psnr=39.7, loss_mc=1.28e-5, loss_ei=9.62e-5, total_loss=0.000109, train_psnr=39.2]    Epoch 1:  60%|██████    | 3/5 [00:08<00:04,  2.17s/it, eval_psnr=39.7, loss_mc=1.48e-5, loss_ei=0.000109, total_loss=0.000123, train_psnr=38.7]    Epoch 1:  80%|████████  | 4/5 [00:08<00:02,  2.17s/it, eval_psnr=39.7, loss_mc=1.48e-5, loss_ei=0.000109, total_loss=0.000123, train_psnr=38.7]    Epoch 1:  80%|████████  | 4/5 [00:08<00:02,  2.17s/it, eval_psnr=39.7, loss_mc=1.48e-5, loss_ei=0.000109, total_loss=0.000123, train_psnr=38.7]    Epoch 1:  80%|████████  | 4/5 [00:10<00:02,  2.17s/it, eval_psnr=39.7, loss_mc=1.34e-5, loss_ei=9.7e-5, total_loss=0.00011, train_psnr=38.5]       Epoch 1: 100%|██████████| 5/5 [00:10<00:00,  2.18s/it, eval_psnr=39.7, loss_mc=1.34e-5, loss_ei=9.7e-5, total_loss=0.00011, train_psnr=38.5]    Epoch 1: 100%|██████████| 5/5 [00:10<00:00,  2.17s/it, eval_psnr=39.7, loss_mc=1.34e-5, loss_ei=9.7e-5, total_loss=0.00011, train_psnr=38.5]

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): HQSIteration(
          (f_step): fStepHQS()
          (g_step): gStepHQS()
        )
      )
      (init_params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0): PnP(
          (denoiser): DnCNN(
            (in_conv): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv_list): ModuleList(
              (0-4): 5 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (out_conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nl_list): ModuleList(
              (0-5): 6 x ReLU()
            )
          )
        )
      )
      (data_fidelity): ModuleList(
        (0): L2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 222-226

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 226-240

.. code-block:: Python


    plot_images = True
    method = "equivariant_imaging"

    test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: No learning, Recons., GT
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/73 [00:00<?, ?it/s]      1%|▏         | 1/73 [00:00<01:06,  1.08it/s]      3%|▎         | 2/73 [00:01<00:32,  2.20it/s]      4%|▍         | 3/73 [00:01<00:20,  3.39it/s]      5%|▌         | 4/73 [00:01<00:15,  4.54it/s]      7%|▋         | 5/73 [00:01<00:12,  5.59it/s]      8%|▊         | 6/73 [00:01<00:10,  6.49it/s]     10%|▉         | 7/73 [00:01<00:09,  7.23it/s]     11%|█         | 8/73 [00:01<00:08,  7.83it/s]     12%|█▏        | 9/73 [00:01<00:07,  8.28it/s]     14%|█▎        | 10/73 [00:01<00:07,  8.62it/s]     15%|█▌        | 11/73 [00:01<00:06,  8.86it/s]     16%|█▋        | 12/73 [00:02<00:06,  8.88it/s]     18%|█▊        | 13/73 [00:02<00:06,  9.05it/s]     19%|█▉        | 14/73 [00:02<00:06,  9.13it/s]     21%|██        | 15/73 [00:02<00:06,  9.23it/s]     22%|██▏       | 16/73 [00:02<00:06,  9.25it/s]     23%|██▎       | 17/73 [00:02<00:06,  9.32it/s]     25%|██▍       | 18/73 [00:02<00:05,  9.36it/s]     26%|██▌       | 19/73 [00:02<00:05,  9.40it/s]     27%|██▋       | 20/73 [00:02<00:05,  9.37it/s]     29%|██▉       | 21/73 [00:03<00:05,  9.34it/s]     30%|███       | 22/73 [00:03<00:05,  9.29it/s]     32%|███▏      | 23/73 [00:03<00:05,  9.34it/s]     33%|███▎      | 24/73 [00:03<00:05,  9.38it/s]     34%|███▍      | 25/73 [00:03<00:05,  9.41it/s]     36%|███▌      | 26/73 [00:03<00:04,  9.41it/s]     37%|███▋      | 27/73 [00:03<00:04,  9.44it/s]     38%|███▊      | 28/73 [00:03<00:04,  9.46it/s]     40%|███▉      | 29/73 [00:03<00:04,  9.46it/s]     41%|████      | 30/73 [00:04<00:04,  9.46it/s]     42%|████▏     | 31/73 [00:04<00:04,  9.29it/s]     44%|████▍     | 32/73 [00:04<00:04,  9.33it/s]     45%|████▌     | 33/73 [00:04<00:04,  9.34it/s]     47%|████▋     | 34/73 [00:04<00:04,  9.38it/s]     48%|████▊     | 35/73 [00:04<00:04,  9.40it/s]     49%|████▉     | 36/73 [00:04<00:03,  9.42it/s]     51%|█████     | 37/73 [00:04<00:03,  9.44it/s]     52%|█████▏    | 38/73 [00:04<00:03,  9.43it/s]     53%|█████▎    | 39/73 [00:04<00:03,  9.44it/s]     55%|█████▍    | 40/73 [00:05<00:03,  9.46it/s]     56%|█████▌    | 41/73 [00:05<00:03,  9.31it/s]     58%|█████▊    | 42/73 [00:05<00:03,  9.35it/s]     59%|█████▉    | 43/73 [00:05<00:03,  9.38it/s]     60%|██████    | 44/73 [00:05<00:03,  9.41it/s]     62%|██████▏   | 45/73 [00:05<00:02,  9.43it/s]     63%|██████▎   | 46/73 [00:05<00:02,  9.44it/s]     64%|██████▍   | 47/73 [00:05<00:02,  9.45it/s]     66%|██████▌   | 48/73 [00:05<00:02,  9.46it/s]     67%|██████▋   | 49/73 [00:06<00:02,  9.47it/s]     68%|██████▊   | 50/73 [00:06<00:02,  9.48it/s]     70%|██████▉   | 51/73 [00:06<00:02,  9.30it/s]     71%|███████   | 52/73 [00:06<00:02,  9.36it/s]     73%|███████▎  | 53/73 [00:06<00:02,  9.38it/s]     74%|███████▍  | 54/73 [00:06<00:02,  9.42it/s]     75%|███████▌  | 55/73 [00:06<00:01,  9.45it/s]     77%|███████▋  | 56/73 [00:06<00:01,  9.46it/s]     78%|███████▊  | 57/73 [00:06<00:01,  9.45it/s]     79%|███████▉  | 58/73 [00:07<00:01,  9.46it/s]     81%|████████  | 59/73 [00:07<00:01,  9.44it/s]     82%|████████▏ | 60/73 [00:07<00:01,  9.37it/s]     84%|████████▎ | 61/73 [00:07<00:01,  9.33it/s]     85%|████████▍ | 62/73 [00:07<00:01,  9.38it/s]     86%|████████▋ | 63/73 [00:07<00:01,  9.40it/s]     88%|████████▊ | 64/73 [00:07<00:00,  9.43it/s]     89%|████████▉ | 65/73 [00:07<00:00,  9.44it/s]     90%|█████████ | 66/73 [00:07<00:00,  9.45it/s]     92%|█████████▏| 67/73 [00:07<00:00,  9.46it/s]     93%|█████████▎| 68/73 [00:08<00:00,  9.38it/s]     95%|█████████▍| 69/73 [00:08<00:00,  9.20it/s]     96%|█████████▌| 70/73 [00:08<00:00,  9.08it/s]     97%|█████████▋| 71/73 [00:08<00:00,  9.06it/s]     99%|█████████▊| 72/73 [00:08<00:00,  9.14it/s]    100%|██████████| 73/73 [00:08<00:00,  9.20it/s]    100%|██████████| 73/73 [00:08<00:00,  8.47it/s]
    Test PSNR: No learning rec.: 29.39+-3.41 dB | Model: 37.38+-2.58 dB. 

    (37.380233163702975, 2.5844099870603792, 29.388851845101133, 3.4114611134712227)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 33.589 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
