
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/unfolded/demo_vanilla_unfolded.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_unfolded_demo_vanilla_unfolded.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_unfolded_demo_vanilla_unfolded.py:


Vanilla Unfolded algorithm for super-resolution
====================================================================================================

This is a simple example to show how to use vanilla unfolded Plug-and-Play.
The DnCNN denoiser and the algorithm parameters (stepsize, regularization parameters) are trained jointly.
For simplicity, we show how to train the algorithm on a  small dataset. For optimal results, use a larger dataset.
For visualizing the training, you can use Weight&Bias (wandb) by setting ``wandb_vis=True``.

.. GENERATED FROM PYTHON SOURCE LINES 10-22

.. code-block:: Python


    import deepinv as dinv
    from pathlib import Path
    import torch
    from torch.utils.data import DataLoader
    from deepinv.optim.data_fidelity import L2
    from deepinv.optim.prior import PnP
    from deepinv.unfolded import unfolded_builder
    from deepinv.training_utils import train, test
    from torchvision import transforms
    from deepinv.utils.demo import load_dataset








.. GENERATED FROM PYTHON SOURCE LINES 23-26

Setup paths for data loading and results.
----------------------------------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 26-38

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 39-42

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------------
In this example, we use the CBSD500 dataset for training and the Set3C dataset for testing.

.. GENERATED FROM PYTHON SOURCE LINES 42-47

.. code-block:: Python


    img_size = 64 if torch.cuda.is_available() else 32
    n_channels = 3  # 3 for color images, 1 for gray-scale images
    operation = "super-resolution"








.. GENERATED FROM PYTHON SOURCE LINES 48-51

Generate a dataset of low resolution images and load it.
----------------------------------------------------------------------------------------
We use the Downsampling class from the physics module to generate a dataset of low resolution images.

.. GENERATED FROM PYTHON SOURCE LINES 51-106

.. code-block:: Python


    # For simplicity, we use a small dataset for training.
    # To be replaced for optimal results. For example, you can use the larger "drunet" dataset.
    train_dataset_name = "CBSD500"
    test_dataset_name = "set3c"
    # Specify the  train and test transforms to be applied to the input images.
    test_transform = transforms.Compose(
        [transforms.CenterCrop(img_size), transforms.ToTensor()]
    )
    train_transform = transforms.Compose(
        [transforms.RandomCrop(img_size), transforms.ToTensor()]
    )
    # Define the base train and test datasets of clean images.
    train_base_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform=train_transform
    )
    test_base_dataset = load_dataset(
        test_dataset_name, ORIGINAL_DATA_DIR, transform=test_transform
    )

    # Use parallel dataloader if using a GPU to fasten training, otherwise, as all computes are on CPU, use synchronous
    # dataloading.
    num_workers = 4 if torch.cuda.is_available() else 0

    # Degradation parameters
    factor = 2
    noise_level_img = 0.03

    # Generate the gaussian blur downsampling operator.
    physics = dinv.physics.Downsampling(
        img_size=(n_channels, img_size, img_size),
        factor=factor,
        mode="gauss",
        device=device,
        noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),
    )
    my_dataset_name = "demo_unfolded_sr"
    n_images_max = (
        1000 if torch.cuda.is_available() else 10
    )  # maximal number of images used for training
    measurement_dir = DATA_DIR / train_dataset_name / operation
    generated_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_base_dataset,
        test_dataset=test_base_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/CBSD500.zip
      0%|          | 0.00/71.0M [00:00<?, ?iB/s]      8%|▊         | 5.83M/71.0M [00:00<00:01, 58.3MiB/s]     18%|█▊        | 13.0M/71.0M [00:00<00:00, 66.2MiB/s]     28%|██▊       | 20.2M/71.0M [00:00<00:00, 68.9MiB/s]     39%|███▉      | 27.6M/71.0M [00:00<00:00, 70.8MiB/s]     49%|████▉     | 35.1M/71.0M [00:00<00:00, 72.4MiB/s]     60%|██████    | 42.7M/71.0M [00:00<00:00, 73.4MiB/s]     71%|███████   | 50.2M/71.0M [00:00<00:00, 74.2MiB/s]     81%|████████▏ | 57.8M/71.0M [00:00<00:00, 74.6MiB/s]     92%|█████████▏| 65.3M/71.0M [00:00<00:00, 74.9MiB/s]    100%|██████████| 71.0M/71.0M [00:00<00:00, 72.9MiB/s]
    CBSD500 dataset downloaded in datasets
    Downloading datasets/set3c.zip
      0%|          | 0.00/385k [00:00<?, ?iB/s]    100%|██████████| 385k/385k [00:00<00:00, 19.8MiB/s]
    set3c dataset downloaded in datasets
    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 50.16it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 109.48it/s]
    Dataset has been saved in measurements/CBSD500/super-resolution




.. GENERATED FROM PYTHON SOURCE LINES 107-114

Define the unfolded PnP algorithm.
----------------------------------------------------------------------------------------
We use the helper function :meth:`deepinv.unfolded.unfolded_builder` to defined the Unfolded architecture.
The chosen algorithm is here DRS (Douglas-Rachford Splitting).
Note that if the prior (resp. a parameter) is initialized with a list of lenght max_iter,
then a distinct model (resp. parameter) is trained for each iteration.
For fixed trained model prior (resp. parameter) across iterations, initialize with a single element.

.. GENERATED FROM PYTHON SOURCE LINES 114-159

.. code-block:: Python


    # Unrolled optimization algorithm parameters
    max_iter = 5  # number of unfolded layers

    # Select the data fidelity term
    data_fidelity = L2()

    # Set up the trainable denoising prior
    # Here the prior model is common for all iterations
    prior = PnP(denoiser=dinv.models.DnCNN(depth=7, pretrained=None, train=True).to(device))

    # The parameters are initialized with a list of length max_iter, so that a distinct parameter is trained for each iteration.
    lamb = [
        1.0
    ] * max_iter  # regularization parameter (multiplier of the data fidelity term)
    stepsize = [1.0] * max_iter  # stepsize of the algorithm
    sigma_denoiser = [0.01] * max_iter  # noise level parameter of the denoiser
    beta = 1.0  # relaxation parameter of the Douglas-Rachford splitting
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "lambda": lamb,
        "g_param": sigma_denoiser,
        "beta": beta,
    }
    trainable_params = [
        "lambda",
        "g_param",
        "stepsize",
        "beta",
    ]  # define which parameters from 'params_algo' are trainable

    # Logging parameters
    verbose = True
    wandb_vis = False  # plot curves and images in Weight&Bias

    # Define the unfolded trainable model.
    model = unfolded_builder(
        iteration="DRS",
        params_algo=params_algo.copy(),
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )








.. GENERATED FROM PYTHON SOURCE LINES 160-163

Define the training parameters.
----------------------------------------------------------------------------------------
We use the Adam optimizer and the StepLR scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 163-185

.. code-block:: Python



    # training parameters
    epochs = 10 if torch.cuda.is_available() else 2
    learning_rate = 5e-4
    train_batch_size = 32 if torch.cuda.is_available() else 1
    test_batch_size = 3

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))

    # choose supervised training loss
    losses = [dinv.loss.SupLoss(metric=dinv.metric.mse())]

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 186-189

Train the network
----------------------------------------------------------------------------------------
We train the network using the library's train function.

.. GENERATED FROM PYTHON SOURCE LINES 189-205

.. code-block:: Python


    train(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,  # training visualization can be done in Weight&Bias
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 188179 trainable parameters
      0%|          | 0/10 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=-35.3, total_loss=518, train_psnr=-27.1]    Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=-35.3, total_loss=518, train_psnr=-27.1]    Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=-35.3, total_loss=260, train_psnr=-14.5]    Epoch 1:  20%|██        | 2/10 [00:00<00:00, 15.37it/s, eval_psnr=-35.3, total_loss=260, train_psnr=-14.5]    Epoch 1:  20%|██        | 2/10 [00:00<00:00, 15.37it/s, eval_psnr=-35.3, total_loss=260, train_psnr=-14.5]    Epoch 1:  20%|██        | 2/10 [00:00<00:00, 15.37it/s, eval_psnr=-35.3, total_loss=173, train_psnr=-9.84]    Epoch 1:  20%|██        | 2/10 [00:00<00:00, 15.37it/s, eval_psnr=-35.3, total_loss=173, train_psnr=-9.84]    Epoch 1:  20%|██        | 2/10 [00:00<00:00, 15.37it/s, eval_psnr=-35.3, total_loss=130, train_psnr=-7.05]    Epoch 1:  40%|████      | 4/10 [00:00<00:00, 15.38it/s, eval_psnr=-35.3, total_loss=130, train_psnr=-7.05]    Epoch 1:  40%|████      | 4/10 [00:00<00:00, 15.38it/s, eval_psnr=-35.3, total_loss=130, train_psnr=-7.05]    Epoch 1:  40%|████      | 4/10 [00:00<00:00, 15.38it/s, eval_psnr=-35.3, total_loss=104, train_psnr=-4.79]    Epoch 1:  40%|████      | 4/10 [00:00<00:00, 15.38it/s, eval_psnr=-35.3, total_loss=104, train_psnr=-4.79]    Epoch 1:  40%|████      | 4/10 [00:00<00:00, 15.38it/s, eval_psnr=-35.3, total_loss=87, train_psnr=-3.63]     Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 14.99it/s, eval_psnr=-35.3, total_loss=87, train_psnr=-3.63]    Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 14.99it/s, eval_psnr=-35.3, total_loss=87, train_psnr=-3.63]    Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 14.99it/s, eval_psnr=-35.3, total_loss=74.7, train_psnr=-2.5]    Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 14.99it/s, eval_psnr=-35.3, total_loss=74.7, train_psnr=-2.5]    Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 14.99it/s, eval_psnr=-35.3, total_loss=65.3, train_psnr=-1.38]    Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 15.15it/s, eval_psnr=-35.3, total_loss=65.3, train_psnr=-1.38]    Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 15.15it/s, eval_psnr=-35.3, total_loss=65.3, train_psnr=-1.38]    Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 15.15it/s, eval_psnr=-35.3, total_loss=58.2, train_psnr=-0.997]    Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 15.15it/s, eval_psnr=-35.3, total_loss=58.2, train_psnr=-0.997]    Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 15.15it/s, eval_psnr=-35.3, total_loss=52.4, train_psnr=0.0833]    Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 15.26it/s, eval_psnr=-35.3, total_loss=52.4, train_psnr=0.0833]    Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 15.22it/s, eval_psnr=-35.3, total_loss=52.4, train_psnr=0.0833]
      0%|          | 0/10 [00:00<?, ?it/s]    Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]    Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=4.54, total_loss=0.133, train_psnr=8.75]    Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=4.54, total_loss=0.133, train_psnr=8.75]    Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s, eval_psnr=4.54, total_loss=0.154, train_psnr=8.16]    Epoch 2:  20%|██        | 2/10 [00:00<00:00, 15.18it/s, eval_psnr=4.54, total_loss=0.154, train_psnr=8.16]    Epoch 2:  20%|██        | 2/10 [00:00<00:00, 15.18it/s, eval_psnr=4.54, total_loss=0.154, train_psnr=8.16]    Epoch 2:  20%|██        | 2/10 [00:00<00:00, 15.18it/s, eval_psnr=4.54, total_loss=0.243, train_psnr=6.69]    Epoch 2:  20%|██        | 2/10 [00:00<00:00, 15.18it/s, eval_psnr=4.54, total_loss=0.243, train_psnr=6.69]    Epoch 2:  20%|██        | 2/10 [00:00<00:00, 15.18it/s, eval_psnr=4.54, total_loss=0.203, train_psnr=7.74]    Epoch 2:  40%|████      | 4/10 [00:00<00:00, 15.27it/s, eval_psnr=4.54, total_loss=0.203, train_psnr=7.74]    Epoch 2:  40%|████      | 4/10 [00:00<00:00, 15.27it/s, eval_psnr=4.54, total_loss=0.203, train_psnr=7.74]    Epoch 2:  40%|████      | 4/10 [00:00<00:00, 15.27it/s, eval_psnr=4.54, total_loss=0.205, train_psnr=7.52]    Epoch 2:  40%|████      | 4/10 [00:00<00:00, 15.27it/s, eval_psnr=4.54, total_loss=0.205, train_psnr=7.52]    Epoch 2:  40%|████      | 4/10 [00:00<00:00, 15.27it/s, eval_psnr=4.54, total_loss=0.194, train_psnr=7.71]    Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 15.30it/s, eval_psnr=4.54, total_loss=0.194, train_psnr=7.71]    Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 15.30it/s, eval_psnr=4.54, total_loss=0.194, train_psnr=7.71]    Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 15.30it/s, eval_psnr=4.54, total_loss=0.179, train_psnr=8.08]    Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 15.30it/s, eval_psnr=4.54, total_loss=0.179, train_psnr=8.08]    Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 15.30it/s, eval_psnr=4.54, total_loss=0.188, train_psnr=7.82]    Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 15.37it/s, eval_psnr=4.54, total_loss=0.188, train_psnr=7.82]    Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 15.37it/s, eval_psnr=4.54, total_loss=0.188, train_psnr=7.82]    Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 15.37it/s, eval_psnr=4.54, total_loss=0.173, train_psnr=8.35]    Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 15.37it/s, eval_psnr=4.54, total_loss=0.173, train_psnr=8.35]    Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 15.37it/s, eval_psnr=4.54, total_loss=0.171, train_psnr=8.32]    Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 15.38it/s, eval_psnr=4.54, total_loss=0.171, train_psnr=8.32]    Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 15.34it/s, eval_psnr=4.54, total_loss=0.171, train_psnr=8.32]

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): DRSIteration(
          (f_step): fStepDRS()
          (g_step): gStepDRS()
        )
      )
      (init_params_algo): ParameterDict(
          (beta): Object of type: ParameterList
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (beta): ParameterList(  (0): Parameter containing: [torch.float32 of size ])
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (beta): Object of type: ParameterList
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (beta): ParameterList(  (0): Parameter containing: [torch.float32 of size ])
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0): PnP(
          (denoiser): DnCNN(
            (in_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv_list): ModuleList(
              (0-4): 5 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (out_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nl_list): ModuleList(
              (0-5): 6 x ReLU()
            )
          )
        )
      )
      (data_fidelity): ModuleList(
        (0): L2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 206-210

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 210-229

.. code-block:: Python


    method = "unfolded_drs"
    save_folder = RESULTS_DIR / method / operation
    wandb_vis = False  # plot curves and images in Weight&Bias.
    plot_images = True  # plot images. Images are saved in save_folder.
    plot_metrics = True  # compute performance and convergence metrics along the algorithm, curved saved in RESULTS_DIR

    test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=save_folder,
        verbose=verbose,
        plot_metrics=plot_metrics,
        wandb_vis=wandb_vis,  # test visualization can be done in Weight&Bias
    )




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_001.png
         :alt: Input, No learning, Recons., GT
         :srcset: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_002.png
         :alt: PSNR, residual
         :srcset: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:03<00:00,  3.11s/it]    100%|██████████| 1/1 [00:03<00:00,  3.11s/it]
    Test PSNR: No learning rec.: 6.00+-0.00 dB | Model: 4.69+-0.00 dB. 

    (4.686464786529541, 0.0, 5.9960274696350098, 0.0)



.. GENERATED FROM PYTHON SOURCE LINES 230-232

Plotting the trained parameters.
------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 232-236

.. code-block:: Python


    dinv.utils.plotting.plot_parameters(
        model, init_params=params_algo, save_dir=RESULTS_DIR / method / operation
    )



.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_003.png
   :alt: demo vanilla unfolded
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_vanilla_unfolded_003.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.715 seconds)


.. _sphx_glr_download_auto_examples_unfolded_demo_vanilla_unfolded.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_vanilla_unfolded.ipynb <demo_vanilla_unfolded.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_vanilla_unfolded.py <demo_vanilla_unfolded.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
